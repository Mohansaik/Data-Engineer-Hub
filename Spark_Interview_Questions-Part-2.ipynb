{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "121fe79a-e825-4874-9971-1972a17406fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 1 (Complex Aggregation)\n",
    "##### Given a DataFrame of employee details with their salaries, find the average salary for each department and the count of employees in each department.\n",
    "\n",
    "**Input data** \n",
    "```\n",
    "+----+-------+---------+------+\n",
    "| ID | Name  | Dept    |Salary|\n",
    "+----+-------+---------+------+\n",
    "| 1  | John  | IT      | 5000 |\n",
    "| 2  | Alice | HR      | 6000 |\n",
    "| 3  | Bob   | IT      | 7000 |\n",
    "| 4  | David | HR      | 5500 |\n",
    "| 5  | Eve   | IT      | 6500 |\n",
    "+----+-------+---------+------+\n",
    "\n",
    "```\n",
    "**Output data**\n",
    "\n",
    "```\n",
    "+---------+-------------+----------------+\n",
    "| Dept    | Avg_Salary  | Employee_Count |\n",
    "+---------+-------------+----------------+\n",
    "| HR      | 5750.0      | 2              |\n",
    "| IT      | 6166.67     | 3              |\n",
    "+---------+-------------+----------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a59939-fcf8-43fe-821e-ab7b3da87cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Dept</th><th>Avg_Salary</th><th>Employee_count</th></tr></thead><tbody><tr><td>IT</td><td>6166.67</td><td>3</td></tr><tr><td>HR</td><td>5750.0</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "IT",
         6166.67,
         3
        ],
        [
         "HR",
         5750.0,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Avg_Salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Employee_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg,count,round \n",
    "data = [\n",
    "    {'ID': 1, 'Name': 'John', 'Dept': 'IT', 'Salary': 5000},\n",
    "    {'ID': 2, 'Name': 'Alice', 'Dept': 'HR', 'Salary': 6000},\n",
    "    {'ID': 3, 'Name': 'Bob', 'Dept': 'IT', 'Salary': 7000},\n",
    "    {'ID': 4, 'Name': 'David', 'Dept': 'HR', 'Salary': 5500},\n",
    "    {'ID': 5, 'Name': 'Eve', 'Dept': 'IT', 'Salary': 6500}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "final_df = df.groupBy('Dept').agg(round(avg('Salary'),2).alias('Avg_Salary'),count('Name').alias('Employee_count'))\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaf89d1e-cf74-410b-8c96-2bcfd233374c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 2 (Window Functions with Partitioning and Ordering)\n",
    "##### Given a DataFrame of sales transactions, calculate the running total of sales for each region, ordered by the transaction date.\n",
    "\n",
    "**Input data** \n",
    "```\n",
    "+--------+-------+-------+------------+\n",
    "| Product| Region| Sales | Date       |\n",
    "+--------+-------+-------+------------+\n",
    "| A      | North | 100   | 2024-01-01 |\n",
    "| A      | North | 200   | 2024-01-02 |\n",
    "| B      | South | 150   | 2024-01-01 |\n",
    "| B      | South | 100   | 2024-01-03 |\n",
    "| C      | North | 300   | 2024-01-02 |\n",
    "| C      | South | 400   | 2024-01-04 |\n",
    "+--------+-------+-------+------------+\n",
    "\n",
    "\n",
    "```\n",
    "**Output data**\n",
    "\n",
    "```\n",
    "+--------+-------+-------+------------+-------------+\n",
    "| Product| Region| Sales | Date       | Running_Sum |\n",
    "+--------+-------+-------+------------+-------------+\n",
    "| A      | North | 100   | 2024-01-01 | 100         |\n",
    "| A      | North | 200   | 2024-01-02 | 300         |\n",
    "| C      | North | 300   | 2024-01-02 | 600         |\n",
    "| B      | South | 150   | 2024-01-01 | 150         |\n",
    "| B      | South | 100   | 2024-01-03 | 250         |\n",
    "| C      | South | 400   | 2024-01-04 | 650         |\n",
    "+--------+-------+-------+------------+-------------+\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1306b267-abb5-480a-879a-3b3c1d4af176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Product</th><th>Region</th><th>Sales</th><th>Date</th><th>Running_Sum</th></tr></thead><tbody><tr><td>A</td><td>North</td><td>100</td><td>2024-01-01</td><td>100</td></tr><tr><td>A</td><td>North</td><td>200</td><td>2024-01-02</td><td>300</td></tr><tr><td>C</td><td>North</td><td>300</td><td>2024-01-02</td><td>600</td></tr><tr><td>B</td><td>South</td><td>150</td><td>2024-01-01</td><td>150</td></tr><tr><td>B</td><td>South</td><td>100</td><td>2024-01-03</td><td>250</td></tr><tr><td>C</td><td>South</td><td>400</td><td>2024-01-04</td><td>650</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         "North",
         100,
         "2024-01-01",
         100
        ],
        [
         "A",
         "North",
         200,
         "2024-01-02",
         300
        ],
        [
         "C",
         "North",
         300,
         "2024-01-02",
         600
        ],
        [
         "B",
         "South",
         150,
         "2024-01-01",
         150
        ],
        [
         "B",
         "South",
         100,
         "2024-01-03",
         250
        ],
        [
         "C",
         "South",
         400,
         "2024-01-04",
         650
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Running_Sum",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum as fsum \n",
    "from pyspark.sql.window import Window\n",
    "data = [\n",
    "    ( 'A', 'North', 100, '2024-01-01'),\n",
    "    ( 'A', 'North', 200, '2024-01-02'),\n",
    "    ( 'B', 'South', 150, '2024-01-01'),\n",
    "    ( 'B', 'South', 100, '2024-01-03'),\n",
    "    ( 'C', 'North', 300, '2024-01-02'),\n",
    "    ( 'C', 'South', 400, '2024-01-04')\n",
    "]\n",
    "columns = ['Product','Region','Sales','Date']\n",
    "\n",
    "df = spark.createDataFrame(data,schema=columns)\n",
    "\n",
    "final_df = df.withColumn('Running_Sum',fsum(col('Sales')).over(Window.partitionBy('Region').orderBy('Date').rowsBetween(Window.unboundedPreceding,Window.currentRow)))\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d234b179-89fd-40b0-9266-a38269883828",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 3 (Complex Joins)\n",
    "##### Given two DataFrames, one with employee details and another with department details, perform a left join to include all employees, even if they donâ€™t have a corresponding department.\n",
    "\n",
    "**Input data (Employee Dataframe)** \n",
    "```\n",
    "+----+-------+---------+\n",
    "| ID | Name  | Dept_ID |\n",
    "+----+-------+---------+\n",
    "| 1  | John  | 101     |\n",
    "| 2  | Alice | 102     |\n",
    "| 3  | Bob   | null    |\n",
    "| 4  | David | 104     |\n",
    "+----+-------+---------+\n",
    "\n",
    "\n",
    "```\n",
    "**Input data (Department DataFrame)** \n",
    "```\n",
    "+--------+------------+\n",
    "| Dept_ID| Dept_Name  |\n",
    "+--------+------------+\n",
    "| 101    | IT         |\n",
    "| 102    | HR         |\n",
    "| 103    | Sales      |\n",
    "+--------+------------+\n",
    "\n",
    "\n",
    "```\n",
    "**Output data**\n",
    "\n",
    "```\n",
    "+----+-------+---------+------------+\n",
    "| ID | Name  | Dept_ID | Dept_Name  |\n",
    "+----+-------+---------+------------+\n",
    "| 1  | John  | 101     | IT         |\n",
    "| 2  | Alice | 102     | HR         |\n",
    "| 3  | Bob   | null    | null       |\n",
    "| 4  | David | 104     | null       |\n",
    "+----+-------+---------+------------+\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6736ff-4549-4770-8d51-c7ba17c4f462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>Name</th><th>Dept_ID</th><th>Dept_Name</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>101</td><td>IT</td></tr><tr><td>2</td><td>Alice</td><td>102</td><td>HR</td></tr><tr><td>3</td><td>Bob</td><td>null</td><td>null</td></tr><tr><td>4</td><td>David</td><td>104</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         101,
         "IT"
        ],
        [
         2,
         "Alice",
         102,
         "HR"
        ],
        [
         3,
         "Bob",
         null,
         null
        ],
        [
         4,
         "David",
         104,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Dept_ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Dept_Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_data = [\n",
    "    {'ID': 1, 'Name': 'John', 'Dept_ID': 101},\n",
    "    {'ID': 2, 'Name': 'Alice', 'Dept_ID': 102},\n",
    "    {'ID': 3, 'Name': 'Bob', 'Dept_ID': None},\n",
    "    {'ID': 4, 'Name': 'David', 'Dept_ID': 104}\n",
    "]\n",
    "\n",
    "department_data = [\n",
    "    {'Dept_ID': 101, 'Dept_Name': 'IT'},\n",
    "    {'Dept_ID': 102, 'Dept_Name': 'HR'},\n",
    "    {'Dept_ID': 103, 'Dept_Name': 'Sales'}\n",
    "]\n",
    "\n",
    "employee_df = spark.createDataFrame(employee_data)\n",
    "\n",
    "department_df = spark.createDataFrame(department_data)\n",
    "\n",
    "final_df = employee_df.join(department_df,on = 'Dept_ID', how='left')\n",
    "\n",
    "display(final_df.select('ID','Name','Dept_ID','Dept_Name'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "707774f7-dc26-4ce2-9750-347f3eb3397e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 4 (DataFrame UDF (User Defined Functions))\n",
    "##### Given a DataFrame of student names, write a UDF to convert their names to uppercase.\n",
    "\n",
    "**Input data** \n",
    "```\n",
    "+----+--------+\n",
    "| ID | Name   |\n",
    "+----+--------+\n",
    "| 1  | John   |\n",
    "| 2  | Alice  |\n",
    "| 3  | Bob    |\n",
    "| 4  | David  |\n",
    "+----+--------+\n",
    "\n",
    "```\n",
    "**Output data**\n",
    "\n",
    "```\n",
    "+----+--------+\n",
    "| ID | Name   |\n",
    "+----+--------+\n",
    "| 1  | JOHN   |\n",
    "| 2  | ALICE  |\n",
    "| 3  | BOB    |\n",
    "| 4  | DAVID  |\n",
    "+----+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51cc3db1-f88b-4a96-b58e-277e5238649f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'ID': 1, 'Name': 'John'},\n",
    "    {'ID': 2, 'Name': 'Alice'},\n",
    "    {'ID': 3, 'Name': 'Bob'},\n",
    "    {'ID': 4, 'Name': 'David'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce547b43-8ebe-4c5b-94bc-ab2351d21a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 5 (Handling Nested Data)\n",
    "##### Given a DataFrame with nested JSON data, flatten the nested structure.\n",
    "\n",
    "**Input data** \n",
    "```\n",
    "+----+---------------------------+\n",
    "| ID | Info                      |\n",
    "+----+---------------------------+\n",
    "| 1  | {\"Name\":\"John\", \"Age\":28} |\n",
    "| 2  | {\"Name\":\"Alice\", \"Age\":34}|\n",
    "| 3  | {\"Name\":\"Bob\", \"Age\":32}  |\n",
    "| 4  | {\"Name\":\"David\", \"Age\":25}|\n",
    "+----+---------------------------+\n",
    "\n",
    "\n",
    "```\n",
    "**Output data**\n",
    "\n",
    "```\n",
    "+----+------+-----+\n",
    "| ID | Name | Age |\n",
    "+----+------+-----+\n",
    "| 1  | John | 28  |\n",
    "| 2  | Alice| 34  |\n",
    "| 3  | Bob  | 32  |\n",
    "| 4  | David| 25  |\n",
    "+----+------+-----+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01fa58ce-37a7-45a6-82df-2f03ac9fd1d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'ID': 1, 'Info': {'Name': 'John', 'Age': 28}},\n",
    "    {'ID': 2, 'Info': {'Name': 'Alice', 'Age': 34}},\n",
    "    {'ID': 3, 'Info': {'Name': 'Bob', 'Age': 32}},\n",
    "    {'ID': 4, 'Info': {'Name': 'David', 'Age': 25}}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2563b84-b345-4f49-92e2-1acab8f5b02a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 6 (Recursive Data Processing)\n",
    "##### Given a DataFrame representing a hierarchy of employees, compute the total salary for each manager, including the salaries of their direct and indirect reports.\n",
    "\n",
    "**Input data** \n",
    "```\n",
    "+----+-------+--------+------+\n",
    "| ID | Name  | Manager|Salary|\n",
    "+----+-------+--------+------+\n",
    "| 1  | John  | null   | 1000 |\n",
    "| 2  | Alice | 1      | 800  |\n",
    "| 3  | Bob   | 1      | 600  |\n",
    "| 4  | David | 2      | 400  |\n",
    "| 5  | Eve   | 3      | 500  |\n",
    "+----+-------+--------+------+\n",
    "\n",
    "\n",
    "```\n",
    "**Output data**\n",
    "\n",
    "```\n",
    "+-------+---------------+\n",
    "| Name  | Total_Salary  |\n",
    "+-------+---------------+\n",
    "| John  | 3300          |\n",
    "| Alice | 1200          |\n",
    "| Bob   | 1100          |\n",
    "| David | 400           |\n",
    "| Eve   | 500           |\n",
    "+-------+---------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45f4a422-fb30-41be-bee7-15885602b3d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'ID': 1, 'Name': 'John', 'Manager': None, 'Salary': 1000},\n",
    "    {'ID': 2, 'Name': 'Alice', 'Manager': 1, 'Salary': 800},\n",
    "    {'ID': 3, 'Name': 'Bob', 'Manager': 1, 'Salary': 600},\n",
    "    {'ID': 4, 'Name': 'David', 'Manager': 2, 'Salary': 400},\n",
    "    {'ID': 5, 'Name': 'Eve', 'Manager': 3, 'Salary': 500}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8c20e1f-1078-4d8d-abb7-ad0176b60792",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 7 (Advanced Window Functions)\n",
    "##### Given a DataFrame of stock prices, calculate the moving average for each stock over a window of 3 days.\n",
    "\n",
    "**Input data** \n",
    "```\n",
    "+--------+---------+----------+------+\n",
    "| Date   | Stock   | Price    | Day  |\n",
    "+--------+---------+----------+------+\n",
    "| 2024-01-01 | AAPL    | 150    | 1   |\n",
    "| 2024-01-02 | AAPL    | 155     | 2  |\n",
    "| 2024-01-03 | AAPL    | 160     | 3  |\n",
    "| 2024-01-04 | AAPL    | 165     | 4  |\n",
    "| 2024-01-05 | AAPL    | 170     | 5  |\n",
    "| 2024-01-01 | GOOG    | 1200    | 1  |\n",
    "| 2024-01-02 | GOOG    | 1220    | 2  |\n",
    "| 2024-01-03 | GOOG    | 1250    | 3  |\n",
    "| 2024-01-04 | GOOG    | 1300    | 4  |\n",
    "| 2024-01-05 | GOOG    | 1350    | 5  |\n",
    "+--------+---------+----------+------+\n",
    "\n",
    "\n",
    "```\n",
    "**Output data**\n",
    "\n",
    "```\n",
    "+--------+---------+----------+-------------+\n",
    "| Date   | Stock   | Price    | Moving_Avg  |\n",
    "+--------+---------+----------+-------------+\n",
    "| 2024-01-03 | AAPL    | 160      | 155.0   |\n",
    "| 2024-01-04 | AAPL    | 165      | 160.0   |\n",
    "| 2024-01-05 | AAPL    | 170      | 165.0   |\n",
    "| 2024-01-03 | GOOG    | 1250     | 1230.0  |\n",
    "| 2024-01-04 | GOOG    | 1300     | 1256.67 |\n",
    "| 2024-01-05 | GOOG    | 1350     | 1300.0  |\n",
    "+--------+---------+----------+-------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7345017-ebc0-4429-8f6c-69e545615c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'Date': '2024-01-01', 'Stock': 'AAPL', 'Price': 150, 'Day': 1},\n",
    "    {'Date': '2024-01-02', 'Stock': 'AAPL', 'Price': 155, 'Day': 2},\n",
    "    {'Date': '2024-01-03', 'Stock': 'AAPL', 'Price': 160, 'Day': 3},\n",
    "    {'Date': '2024-01-04', 'Stock': 'AAPL', 'Price': 165, 'Day': 4},\n",
    "    {'Date': '2024-01-05', 'Stock': 'AAPL', 'Price': 170, 'Day': 5},\n",
    "    {'Date': '2024-01-01', 'Stock': 'GOOG', 'Price': 1200, 'Day': 1},\n",
    "    {'Date': '2024-01-02', 'Stock': 'GOOG', 'Price': 1220, 'Day': 2},\n",
    "    {'Date': '2024-01-03', 'Stock': 'GOOG', 'Price': 1250, 'Day': 3},\n",
    "    {'Date': '2024-01-04', 'Stock': 'GOOG', 'Price': 1300, 'Day': 4},\n",
    "    {'Date': '2024-01-05', 'Stock': 'GOOG', 'Price': 1350, 'Day': 5}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eefaeb72-432a-4bfd-8ce5-ca8f5bb5bfe1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Question 8 (Handling Skewed Data with Salting)\n",
    "##### Given a highly skewed DataFrame with many transactions for certain products, use salting to repartition the data and balance the load across partitions.\n",
    "\n",
    "**Input data** \n",
    "```\n",
    "+--------+-------+------+\n",
    "| Product| Region| Sales|\n",
    "+--------+-------+------+\n",
    "| A      | North | 100  |\n",
    "| A      | North | 200  |\n",
    "| A      | North | 150  |\n",
    "| B      | South | 300  |\n",
    "| C      | North | 400  |\n",
    "| C      | South | 500  |\n",
    "| C      | South | 600  |\n",
    "+--------+-------+------+\n",
    "\n",
    "```\n",
    "**Expected Output: (The output is repartitioned such that the data distribution across partitions is balanced)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f08ade2-7c98-488c-94c7-baa438239222",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'Product': 'A', 'Region': 'North', 'Sales': 100},\n",
    "    {'Product': 'A', 'Region': 'North', 'Sales': 200},\n",
    "    {'Product': 'A', 'Region': 'North', 'Sales': 150},\n",
    "    {'Product': 'B', 'Region': 'South', 'Sales': 300},\n",
    "    {'Product': 'C', 'Region': 'North', 'Sales': 400},\n",
    "    {'Product': 'C', 'Region': 'South', 'Sales': 500},\n",
    "    {'Product': 'C', 'Region': 'South', 'Sales': 600}\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_Interview_Questions-Part-2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
